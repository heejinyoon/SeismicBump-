{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "septis_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heejinyoon/SeismicBump-/blob/main/septis_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeuaBB_A6qdL"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import io\n",
        "import requests\n",
        "\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "from plotly import tools\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from datetime import tzinfo, timedelta, datetime, date\n",
        "\n",
        "import os\n",
        "from os.path import join, getsize\n",
        "from pathlib import Path\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geUoIOOAtg3R"
      },
      "source": [
        "# import SKLearn libs\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, train_test_split, StratifiedKFold, cross_validate, StratifiedShuffleSplit\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z18s3vDEdoi"
      },
      "source": [
        "Early Prediction of Sepsis from Clinical Data -- the PhysioNet Computing in Cardiology Challenge 2019\n",
        "Matthew Reyna  ,  Chris Josef  ,  Russell Jeter  ,  Supreeth Shashikumar  ,  Benjamin Moody  ,  M. Brandon Westover  ,  Ashish Sharma  ,  Shamim Nemati  ,  Gari Clifford \n",
        "\n",
        "Published: Aug. 5, 2019. Version: 1.0.0\n",
        "https://physionet.org/content/challenge-2019/1.0.0/\n",
        "\n",
        "Creative Commons Attribution 4.0 International Public License\n",
        "\n",
        "https://archive.physionet.org/users/shared/challenge-2019/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E80YRSZevyT1",
        "outputId": "af4de2f4-f0a8-4f26-a784-0ef1084f9785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "training_setAzip = \"https://archive.physionet.org/users/shared/challenge-2019/training_setA.zip\"\n",
        "training_setBzip = \"https://archive.physionet.org/users/shared/challenge-2019/training_setB.zip\"\n",
        "\n",
        "!wget {training_setAzip} \n",
        "!wget {training_setBzip} \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-06 19:22:01--  https://archive.physionet.org/users/shared/challenge-2019/training_setA.zip\n",
            "Resolving archive.physionet.org (archive.physionet.org)... 128.30.30.88\n",
            "Connecting to archive.physionet.org (archive.physionet.org)|128.30.30.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22991842 (22M) [application/zip]\n",
            "Saving to: ‘training_setA.zip.1’\n",
            "\n",
            "training_setA.zip.1 100%[===================>]  21.93M  24.4MB/s    in 0.9s    \n",
            "\n",
            "2020-09-06 19:22:02 (24.4 MB/s) - ‘training_setA.zip.1’ saved [22991842/22991842]\n",
            "\n",
            "--2020-09-06 19:22:02--  https://archive.physionet.org/users/shared/challenge-2019/training_setB.zip\n",
            "Resolving archive.physionet.org (archive.physionet.org)... 128.30.30.88\n",
            "Connecting to archive.physionet.org (archive.physionet.org)|128.30.30.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20000482 (19M) [application/zip]\n",
            "Saving to: ‘training_setB.zip.1’\n",
            "\n",
            "training_setB.zip.1 100%[===================>]  19.07M  23.7MB/s    in 0.8s    \n",
            "\n",
            "2020-09-06 19:22:03 (23.7 MB/s) - ‘training_setB.zip.1’ saved [20000482/20000482]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xla2DOGqG6SZ"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# set up local directory\n",
        "base_dir = Path(\"septis\")\n",
        "base_dir.mkdir(exist_ok=True)\n",
        "base_dirA = Path(\"septis/setA\")\n",
        "base_dirA.mkdir(exist_ok=True)\n",
        "pathA = \"/content/\" + str( base_dirA)\n",
        "\n",
        "base_dirB = Path(\"septis/setB\")\n",
        "base_dirB.mkdir(exist_ok=True)\n",
        "pathB = \"/content/\" + str( base_dirB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P13UFjUh1rFp",
        "outputId": "d2237a84-8f24-4d94-8958-af90f88db686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "!unzip \"/content/training_setA.zip\" -d {pathA}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/training_setA.zip\n",
            "replace /content/septis/setA/training/p000001.psv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73mqgs6LnDB_"
      },
      "source": [
        "path = \"/content/septis\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddXbnvDxehSb"
      },
      "source": [
        "!unzip \"/content/training_setB.zip\" -d {pathB}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tytPVGPuFZsj"
      },
      "source": [
        "# Have a look at 1 file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYuK3lSSFZMM"
      },
      "source": [
        "fullpath = \"/content/septis/setB/training_setB/p104592.psv\"\n",
        "patientdata = pd.read_csv(fullpath, sep='|', na_values='NaN')\n",
        "patientdata.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fFVtYvtFqph"
      },
      "source": [
        "patientdata.loc[:, ['ICULOS', 'SepsisLabel']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnjPjMCHFMi6"
      },
      "source": [
        "# Load all files raw data into 1 Pandas frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9tMn3wMr-mH"
      },
      "source": [
        "# Load the 1st (2*patientfilesmax) patient files (50 with no septis and 50 with septis) to understand data better\n",
        "noseptispatientfiles = []\n",
        "noseptispatientcount = 0\n",
        "septispatientfiles = []\n",
        "septispatientcount = 0\n",
        "noseptis_patientfilesmax = 200\n",
        "septis_patientfilesmax = 800\n",
        "patientids = []\n",
        "patients = []\n",
        "\n",
        "for dirname, dirs, filenames in os.walk('/content/septis/'):\n",
        "    print(dirname, \"consumes\", end=\" \")\n",
        "    #print(sum(getsize(join(dirname, name)) for name in filenames), end=\" \")\n",
        "    print(\"bytes in\", len(filenames), \"non-directory files\")\n",
        "    for filename in filenames[0:10000]:\n",
        "        fullpath = os.path.join(dirname, filename)\n",
        "        patientdata = pd.read_csv(fullpath, sep='|', na_values='NaN')\n",
        "        if ((patientdata['SepsisLabel'].max() == 0) & (noseptispatientcount < noseptis_patientfilesmax)):\n",
        "          noseptispatientcount = noseptispatientcount + 1\n",
        "          patients.append(patientdata)\n",
        "          patientids.append(filename)\n",
        "          #print(\"No septis: \" + fullpath)\n",
        "        elif ((patientdata['SepsisLabel'].max() == 1) & (septispatientcount < septis_patientfilesmax)):\n",
        "          septispatientcount = septispatientcount + 1\n",
        "          patients.append(patientdata)\n",
        "          patientids.append(filename)\n",
        "          #print(\"Septis: \" + fullpath)\n",
        "df = pd.concat(patients, keys=patientids)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88sNO5m1u95v"
      },
      "source": [
        "print(\"septispatientcount: \" + str(septispatientcount) + \" noseptispatientcount: \" + str(noseptispatientcount))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2UOifOxMgWw"
      },
      "source": [
        "df.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvZI0ir_7KD_"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7jJWbSrCCeW"
      },
      "source": [
        "df.reset_index(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liEhQXhOCU7c"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoffRKkjXa_T"
      },
      "source": [
        "df_description = df.describe().T\n",
        "df_description"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfMWF26oYsKh"
      },
      "source": [
        "df.describe(include=['object']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPh-TjqrSJhO"
      },
      "source": [
        "# Find which columns have a lot of Null values\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3wB-fAtmPkr"
      },
      "source": [
        "plt.hist(df['Temp'], bins=20)\n",
        "plt.title(\"Histogram\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dHoPil5mK6e"
      },
      "source": [
        "sns.countplot(df['SepsisLabel'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x2tpYbEj8sK"
      },
      "source": [
        "df['Temp']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6OfPr2221-z"
      },
      "source": [
        "# Luciana"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt5oRN2CS9qd"
      },
      "source": [
        "#Handling Outliers(data samples distant from all other observations)\n",
        "\n",
        "outliers=[]\n",
        "def outliers_noplotdetection(df):\n",
        "  threshold=3\n",
        "  mean_score = np.mean(df)\n",
        "  std_score = np.std(df)\n",
        "\n",
        "  for df_sample in df:\n",
        "    z_score = (df_sample - mean_score)/std_score\n",
        "    if np.abs(z_score)>threshold:\n",
        "      outliers.append(df_sample)\n",
        "  return outliers\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVJv1s56iFdX"
      },
      "source": [
        "mean_score = np.mean(df)\n",
        "mean_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXqYO9EUhdU0"
      },
      "source": [
        "#outliers = outliers_noplotdetection(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyHQY7PK2ThC"
      },
      "source": [
        "# Catherine section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVd4dw17s0zh"
      },
      "source": [
        "## Vital Signs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKQkcwxgTA_u"
      },
      "source": [
        "# Analyze Vital Signs measuremnts and correlation with Sepsis\n",
        "label = 'SepsisLabel'\n",
        "col_list_categorical = ['level_0'] # patient code\n",
        "col_list_vital_signs = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2']\n",
        "df[col_list_vital_signs].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94UFw3ketQxc"
      },
      "source": [
        "# ignore EtCO2 as very value\n",
        "col_list_for_corr = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'level_1', 'SepsisLabel']\n",
        "df_corr = df[col_list_for_corr].corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0MYrEqptXgR"
      },
      "source": [
        "plt.figure(figsize=[8, 8])\n",
        "sns.heatmap(data=df_corr, vmin=-1, vmax=1, cmap='gist_earth_r', annot=True, square=True, linewidths=1)\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swnbMtjbBKuo"
      },
      "source": [
        "### Very small correlation. \n",
        "> Level_1 (time since entry to ICU, same as ICULOS) has slight correlation as might be expected (patient might get worse as time passes)\n",
        "\n",
        "> ignore EtCO2 as no value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOIFFwS6Ert3"
      },
      "source": [
        "## Demographics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvuWmWJEAiLT"
      },
      "source": [
        "col_list_demographics = ['Age', 'Gender', 'Unit1', 'Unit2', 'HospAdmTime', 'ICULOS']\n",
        "df[col_list_demographics].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJTVAq8tAwPT"
      },
      "source": [
        "col_list_for_corr = ['Age', 'Gender', 'Unit1', 'Unit2', 'HospAdmTime', 'ICULOS', 'SepsisLabel']\n",
        "df_corr = df[col_list_for_corr].corr()\n",
        "\n",
        "plt.figure(figsize=[8, 8])\n",
        "sns.heatmap(data=df_corr, vmin=-1, vmax=1, cmap='gist_earth_r', annot=True, square=True, linewidths=1)\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcTzf5exCG5O"
      },
      "source": [
        "### Very small correlation again. \n",
        "> Level_1 (time since entry to ICU, same as ICULOS) has slight correlation as might be expected (patient might get worse as time passes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YvABsuXCXbD"
      },
      "source": [
        "## Laboratory measures "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuiTXYtZCPU9"
      },
      "source": [
        "col_list_Laboratory = ['BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2',\n",
        "       'AST', 'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine',\n",
        "       'Bilirubin_direct', 'Glucose', 'Lactate', 'Magnesium', 'Phosphate',\n",
        "       'Potassium', 'Bilirubin_total', 'TroponinI', 'Hct', 'Hgb', 'PTT', 'WBC',\n",
        "       'Fibrinogen', 'Platelets']\n",
        "df[col_list_Laboratory].describe().T.sort_values(by='count', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1wVJB3xChqY"
      },
      "source": [
        "col_list_for_corr = ['BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2',\n",
        "       'AST', 'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine',\n",
        "       'Glucose', 'Lactate', 'Magnesium', 'Phosphate',\n",
        "       'Potassium', 'Bilirubin_total', 'Hct', 'Hgb', 'PTT', 'WBC',\n",
        "       'Fibrinogen', 'Platelets', 'SepsisLabel']  \n",
        "df_corr = df[col_list_for_corr].corr()\n",
        "\n",
        "plt.figure(figsize=[16, 16])\n",
        "sns.heatmap(data=df_corr, vmin=-1, vmax=1, cmap='gist_earth_r', annot=True, square=True, linewidths=1)\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH9hr6B9Dd33"
      },
      "source": [
        "### Very sparse date, and small correlation again.\n",
        "\n",
        "> Ignore 'Bilirubin_direct', 'TroponinI', as very few values\n",
        "\n",
        "> Hgb and Hct redundant?\n",
        "\n",
        "\n",
        "> white blood cells (WBCs) correlation makes sense , as reaction to infection.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tylRNSru8SC9"
      },
      "source": [
        "## Forward fill measures per patient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBRyKPdT1bDQ"
      },
      "source": [
        "col_to_fill = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', \n",
        "       'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST', 'BUN',\n",
        "       'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine', 'Bilirubin_direct',\n",
        "       'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium',\n",
        "       'Bilirubin_total', 'TroponinI', 'Hct', 'PTT', 'WBC',\n",
        "       'Fibrinogen', 'Platelets']\n",
        "\n",
        "col_filled = ['Age', 'Gender', 'HospAdmTime', 'ICULOS']\n",
        "\n",
        "col_ignore = ['level_0', 'level_1', 'Unit1', 'Unit2', 'Hgb', 'EtCO2'] \n",
        "col_delta = ['HR_delta', 'Temp_delta', 'O2Sat_delta']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXi0N2aR1o2E"
      },
      "source": [
        "# Returns patientdata with filled values\n",
        "def fillMissingValues(filepath):\n",
        "  patientdata = pd.read_csv(filepath, sep='|', na_values='NaN')\n",
        "\n",
        "  # mark where there where Null values\n",
        "  #cols_with_missing = [col for col in col_to_fill \n",
        "  #                     if patientdata[col].isnull().any()]\n",
        "\n",
        "  for col in col_to_fill:\n",
        "    patientdata[col + '_was_missing'] = patientdata[col].isnull()\n",
        "\n",
        "  # Forward fill available values\n",
        "  patientdata.fillna(method='ffill', inplace=True)\n",
        "\n",
        "  # Delta values: diff with previous records\n",
        "  patientdata[\"HR_delta\"] = patientdata[\"HR\"].diff(periods=1)\n",
        "  patientdata[\"Temp_delta\"] = patientdata[\"Temp\"].diff(periods=1)\n",
        "  patientdata[\"O2Sat_delta\"] = patientdata[\"O2Sat\"].diff(periods=1)\n",
        "\n",
        "  # Fill remaining values with mean values from original data\n",
        "  for col in col_to_fill:\n",
        "    values = {col: df_description['mean'][col]}\n",
        "    patientdata.fillna(value=values, inplace=True)  \n",
        "\n",
        "  for col in col_delta:\n",
        "    values = {col: 0}\n",
        "    patientdata.fillna(value=values, inplace=True) \n",
        "\n",
        "  return patientdata\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgSXOYFylDA-"
      },
      "source": [
        "fullpath = \"/content/septis/setB/training_setB/p104592.psv\"\n",
        "dt = fillMissingValues(fullpath)\n",
        "dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfYqE-VqDyRt"
      },
      "source": [
        " # Load the 1st 100 patient files (50 with no septis and 50 with septis) to understand data better\n",
        "noseptispatientfiles = []\n",
        "noseptispatientcount = 0\n",
        "septispatientfiles = []\n",
        "septispatientcount = 0\n",
        "noseptis_patientfilesmax = 100\n",
        "septis_patientfilesmax = 800\n",
        "patientids = []\n",
        "patients = []\n",
        "\n",
        "no_septis_tests_files=[]\n",
        "septis_tests_files=[]\n",
        "\n",
        "for dirname, dirs, filenames in os.walk('/content/septis/'):\n",
        "    print(dirname, \"consumes\", end=\" \")\n",
        "    #print(sum(getsize(join(dirname, name)) for name in filenames), end=\" \")\n",
        "    print(\"bytes in\", len(filenames), \"non-directory files\")\n",
        "    for filename in filenames[0:6000]:\n",
        "        fullpath = os.path.join(dirname, filename)\n",
        "        patientdata = fillMissingValues(fullpath)\n",
        "        if (patientdata['SepsisLabel'].max() == 0):\n",
        "          if (noseptispatientcount < noseptis_patientfilesmax):\n",
        "            noseptispatientcount = noseptispatientcount + 1\n",
        "            patients.append(patientdata)\n",
        "            patientids.append(filename)\n",
        "            #print(\"No septis: \" + fullpath)\n",
        "          else:\n",
        "            no_septis_tests_files.append(fullpath)\n",
        "        elif (patientdata['SepsisLabel'].max() == 1):\n",
        "          if (septispatientcount < septis_patientfilesmax):\n",
        "            septispatientcount = septispatientcount + 1\n",
        "            patients.append(patientdata)\n",
        "            patientids.append(filename)\n",
        "            #print(\"Septis: \" + fullpath)\n",
        "          else:\n",
        "            septis_tests_files.append(fullpath)\n",
        "\n",
        "df_ffill = pd.concat(patients, keys=patientids)\n",
        "\n",
        "print(\"septispatientcount: \" + str(septispatientcount) + \" noseptispatientcount: \" + str(noseptispatientcount))\n",
        "\n",
        "df_ffill.reset_index(inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq_tF7xJx5Vu"
      },
      "source": [
        "print(\"septispatient test files: \" + str(len(septis_tests_files)) + \n",
        "      \" no septispatient test files: \" + str(len(no_septis_tests_files)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qljgwdi3lwl9"
      },
      "source": [
        "cols_with_missing = [o for o in df_ffill.columns if o.endswith('_was_missing')]\n",
        "cols_with_missing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9BTDKFj84Li"
      },
      "source": [
        "# Compare number of Null Values of Forward-filled data with Raw Data\n",
        "\n",
        "df_ffill_description = df_ffill.describe().T.sort_values(by='count', ascending=False)\n",
        "ffilnull = df_ffill.isnull().sum()\n",
        "dfnull = df.isnull().sum()\n",
        "nulldf = pd.DataFrame({'rawdf': dfnull, 'ffilldf': ffilnull})\n",
        "nulldf.head(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBshN_3XpAM3"
      },
      "source": [
        "nulldf.tail(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqvFp49_9fhO"
      },
      "source": [
        "df_ffill.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf6TQWiSsUaz"
      },
      "source": [
        "# Check no colun missed!\n",
        "print(\"Colums num: \" + str( len(col_to_fill) + len(col_filled) + len(col_delta)  + len(col_ignore) + len(cols_with_missing) + 1) + \" : \" + str(len(df_ffill.columns)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTpBEoRItBaT"
      },
      "source": [
        "## Set-up X and y datests for learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmh4O3e1pnAR"
      },
      "source": [
        "Xcols = (col_to_fill + col_filled + col_delta + cols_with_missing)\n",
        "X = (df_ffill.loc[:, Xcols]).copy()\n",
        "\n",
        "y = df_ffill.SepsisLabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWaEEBAZtOft"
      },
      "source": [
        "# spliting data set in ratio 8:2 for training and testing \n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLWBQBJnMM8-"
      },
      "source": [
        "print('Training Set:')\n",
        "print('Features', train_X.shape, '\\nLabel', train_y.shape)\n",
        "print('Testing Set:')\n",
        "print('Features', val_X.shape, '\\nLabel', val_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OriHlCW0tpQV"
      },
      "source": [
        "# fit and transform data \n",
        "sc = StandardScaler()\n",
        "sc.fit_transform(train_X)\n",
        "sc.transform(val_X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MCiDbxguDUD"
      },
      "source": [
        "## Try Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpSfeJ10t1Iv"
      },
      "source": [
        "rfc = RandomForestClassifier(n_estimators=800)\n",
        "rfc.fit(train_X, train_y)\n",
        "pred_rfc = rfc.predict(val_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vhdqDo0t-j_"
      },
      "source": [
        "print(classification_report(val_y, pred_rfc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA1CY93BunBL"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "      -                    Actual Positive      Actual Negative\n",
        "      Predictive Positive  True Positive        False Positive\n",
        "      Predicted Negative.  False Negative.      True Negative\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghqcpjrSutdC"
      },
      "source": [
        "print(confusion_matrix(val_y, pred_rfc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgfkMtpa_GQl"
      },
      "source": [
        "from pprint import pprint\n",
        "# Look at parameters used by our current forest\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(rfc.get_params())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WISV8TSLsX1W"
      },
      "source": [
        "train_X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHRRgC2k_JAG"
      },
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "pprint(random_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVF2pnhPD6EW"
      },
      "source": [
        "## TODO: finetune Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIXAVYKJ_OVU"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "#rfc = RandomForestClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "#rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, \n",
        "#                               n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "#rfc_random.fit(train_X, train_y)\n",
        "\n",
        "#rfc_random.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clWff-JixU1I"
      },
      "source": [
        "#rfc_random.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqAzAcuD05zo"
      },
      "source": [
        "# VERY LONG optimisation , comment out , results here:\n",
        "{'bootstrap': False,\n",
        " 'max_depth': None,\n",
        " 'max_features': 'auto',\n",
        " 'min_samples_leaf': 1,\n",
        " 'min_samples_split': 2,\n",
        " 'n_estimators': 1100}\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
        "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
        "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 61.3min\n",
        "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 247.6min\n",
        "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 509.2min finished\n",
        "RandomizedSearchCV(cv=3, error_score=nan,\n",
        "                   estimator=RandomForestClassifier(bootstrap=True,\n",
        "                                                    ccp_alpha=0.0,\n",
        "                                                    class_weight=None,\n",
        "                                                    criterion='gini',\n",
        "                                                    max_depth=None,\n",
        "                                                    max_features='auto',\n",
        "                                                    max_leaf_nodes=None,\n",
        "                                                    max_samples=None,\n",
        "                                                    min_impurity_decrease=0.0,\n",
        "                                                    min_impurity_split=None,\n",
        "                                                    min_samples_leaf=1,\n",
        "                                                    min_samples_split=2,\n",
        "                                                    min_weight_fraction_leaf=0.0,\n",
        "                                                    n_estimators=100,\n",
        "                                                    n_jobs...\n",
        "                                                    warm_start=False),\n",
        "                   iid='deprecated', n_iter=100, n_jobs=-1,\n",
        "                   param_distributions={'bootstrap': [True, False],\n",
        "                                        'max_depth': [10, 32, 55, 77, 100,\n",
        "                                                      None],\n",
        "                                        'max_features': ['auto', 'sqrt'],\n",
        "                                        'min_samples_leaf': [1, 2, 4],\n",
        "                                        'min_samples_split': [2, 5, 10],\n",
        "                                        'n_estimators': [200, 650, 1100, 1550,\n",
        "                                                         2000]},\n",
        "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
        "                   return_train_score=False, scoring=None, verbose=2)\n",
        "\n",
        "# This is formatted as code\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqTUw_LB09AL"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "#rfc = RandomForestClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "#rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, \n",
        "#                               n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "#rfc_random.fit(train_X, train_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdP9q15E1P2P"
      },
      "source": [
        "best_rfc = RandomForestClassifier(bootstrap = False, max_depth = None, max_features = 'sqrt',\n",
        "                                  min_samples_leaf = 1, min_samples_split = 2, n_estimators = 1100)\n",
        "best_rfc.fit(train_X, train_y)\n",
        "best_pred_rfc = best_rfc.predict(val_X)\n",
        "\n",
        "{'bootstrap': False,\n",
        " 'max_depth': None,\n",
        " 'max_features': 'auto',\n",
        " 'min_samples_leaf': 1,\n",
        " 'min_samples_split': 2,\n",
        " 'n_estimators': 1100}\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMBZzi9Z10Jj"
      },
      "source": [
        "print(classification_report(val_y, best_pred_rfc))\n",
        "\n",
        "print(confusion_matrix(val_y, best_pred_rfc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmXrJFBgUPf0"
      },
      "source": [
        "\n",
        "8445 / (8445 + 310)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4drkNwxl3Ag9"
      },
      "source": [
        "Non fine-tuned RFC\n",
        "\n",
        "```\n",
        "[[8243   83]\n",
        " [ 331 1223]]\n",
        "```\n",
        "\n",
        "With Null columns\n",
        "\n",
        "```\n",
        "[[8457   76]\n",
        " [ 450 1126]]\n",
        "```\n",
        "\n",
        "Fined-tuned RFC\n",
        "\n",
        "\n",
        "```\n",
        "[[8241   85]\n",
        " [ 232 1322]]\n",
        "```\n",
        "\n",
        "\n",
        "Fined-tuned RFC with NULL columns\n",
        "```\n",
        "[[8445   88]\n",
        " [ 310 1266]]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IksCvJscOnSh"
      },
      "source": [
        "#RandomForestClassifier(bootstrap = False, max_depth = None, max_features = 'sqrt',\n",
        "#                                  min_samples_leaf = 1, min_samples_split = 2, n_estimators = 1100)\n",
        "#best_rfc.fit(train_X, train_y)\n",
        "#best_pred_rfc = best_rfc.predict(val_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaaIPMOHOfna"
      },
      "source": [
        "def rf(xs, y, n_estimators=100, \n",
        "       max_features='sqrt', min_samples_leaf=5, **kwargs):\n",
        "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
        "        max_features=max_features,\n",
        "        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wajRpIdPpmq"
      },
      "source": [
        "m = rf(train_X, train_y);\n",
        "m_pred = m.predict(val_X)\n",
        "cm = confusion_matrix(val_y, m_pred)\n",
        "print(cm)\n",
        "cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYJjY7z6i4MK"
      },
      "source": [
        "preds = np.stack([t.predict(val_X) for t in m.estimators_])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWAD_ud0fmpX"
      },
      "source": [
        "def myrecall(y_true, y_pred):\n",
        "  class_pred = [1 if t >=0.5 else 0 for t in y_pred]\n",
        "  cm = confusion_matrix(y_true, class_pred)\n",
        "  #print(cm)\n",
        "  return (cm[1, 1] / (cm[1, 1] + cm[1, 0]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C5SUQiDcu0O"
      },
      "source": [
        "plt.plot([myrecall(val_y, preds[:i+1].mean(0)) for i in range(100)]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0xd5qiCibnh"
      },
      "source": [
        "m = rf(train_X, train_y, max_features=\"auto\")\n",
        "preds = np.stack([t.predict(val_X) for t in m.estimators_])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o8HOoDzjDGr"
      },
      "source": [
        "plt.plot([myrecall(val_y, preds[:i+1].mean(0)) for i in range(100)]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puuD79yYuy-7"
      },
      "source": [
        "## Pretty good results :=) \n",
        "> now, what are the most important model features in this model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhKPenLnvIQI"
      },
      "source": [
        "importances = best_rfc.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in best_rfc.estimators_], axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        " \n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        " \n",
        "for f in range(train_X.shape[1]):\n",
        "    print(\"%d. feature %d: %s (%f)\" % (f + 1, indices[f], train_X.columns[f], importances[indices[f]]))\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYKs4IWAvSrC"
      },
      "source": [
        "## HR Heart Rate is the Top feature in that model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDY5zJI326XO"
      },
      "source": [
        "# Heejin Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDnoe_06BEJ_"
      },
      "source": [
        "# Missing value visualization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBUh30blb_6I"
      },
      "source": [
        "import missingno as msno \n",
        "lab_df = df[col_list_Laboratory]\n",
        "vital_df = df[col_list_vital_signs]\n",
        "demograph_df = df[col_list_demographics]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk3DiCN_dKlX"
      },
      "source": [
        "np.mean((lab_df.isnull().sum() / lab_df.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5DWIr88dYDD"
      },
      "source": [
        "np.mean((vital_df.isnull().sum() / vital_df.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q73ylt_rdk8_"
      },
      "source": [
        "np.mean((demograph_df.isnull().sum() / demograph_df.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsphRw5wdumP"
      },
      "source": [
        "msno.matrix(lab_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7aPRsv0dzeB"
      },
      "source": [
        "msno.matrix(vital_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxKMoY-nd22r"
      },
      "source": [
        "msno.matrix(demograph_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNr5eHz4d7oK"
      },
      "source": [
        "missingDataDf = df.columns[df.isnull().any()].tolist()\n",
        "msno.bar(df[missingDataDf], color=\"Red\", log=False, figsize=(35,13))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MM2nQ3eqbc"
      },
      "source": [
        "msno.heatmap(df[missingDataDf], figsize=(25,25))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_5XGfuiBJAd"
      },
      "source": [
        "# Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqKqzhRKtzH8"
      },
      "source": [
        "df.level_0.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4EptOcsUJkk"
      },
      "source": [
        "#subplots\n",
        "plt.figure(figsize = (10,8))\n",
        "\n",
        "#KDE plot - smoothed histograms showing distribution of a variable for sepsis \n",
        "#           patients/non-sepsis patients \n",
        "patientsdf = df.groupby('level_0').agg(\n",
        "    min_ICULOS = ('ICULOS', min),\n",
        "    max_ICULOS = ('ICULOS', max),\n",
        "    min_SepsisLabel = ('SepsisLabel', min),\n",
        "    max_SepsisLabel = ('SepsisLabel', max),\n",
        "    min_HeartRate = ('HR', min),\n",
        "    max_HeartRate = ('HR', max),\n",
        "    min_Temperature = ('Temp', min),\n",
        "    max_Temperature = ('Temp', max),\n",
        "    Age = ('Age', min),\n",
        "    Gender = ('Gender', min)\n",
        ")\n",
        "# check there are 100 patients who ended up having sepsis at some point and 100 who didn't\n",
        "#patientsdf.max_SepsisLabel.value_counts()\n",
        "\n",
        "sns.kdeplot(patientsdf.loc[patientsdf['max_SepsisLabel'] == 0]['Age'], label = 'Non-sepsis Patient')\n",
        "sns.kdeplot(patientsdf.loc[patientsdf['max_SepsisLabel'] == 1]['Age'], label = 'Sepsis Patient')\n",
        "\n",
        "#Labeling of plot \n",
        "plt.xlabel('Age (years)'); plt.ylabel('Sepsis'); plt.title('Distribution of Ages')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b22TIbbC7wiI"
      },
      "source": [
        "patientsdf.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwuEBvPnvdvu"
      },
      "source": [
        "plt.hist(patientsdf['max_ICULOS'], bins=20)\n",
        "plt.title(\"Max Number of Hours in ICU Histogram\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il676GAzC4yB"
      },
      "source": [
        "f, ax = plt.subplots(2,2, figsize=(15,10))\n",
        "sns.countplot(x = patientsdf[\"max_SepsisLabel\"], palette = 'Blues', ax=ax[0,0])\n",
        "ax[0,0].set_title('Count of patients who develop sepsis (1) vs. don\\'t (0) ')\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "sns.countplot(x = df[\"SepsisLabel\"], palette = 'Blues', ax=ax[0,1])\n",
        "ax[0,1].set_title('Count of total time slots with sepsis ')\n",
        "\n",
        "ax[1,0].hist(patientsdf.loc[patientsdf['max_SepsisLabel'] == 0]['max_ICULOS'], bins=20, )\n",
        "ax[1,0].set_title(\"Distribution of max number of hours in ICU for patients who don't develop Sespis\")\n",
        "\n",
        "ax[1,1].hist(patientsdf.loc[patientsdf['max_SepsisLabel'] == 1]['max_ICULOS'], bins=20, )\n",
        "ax[1,1].set_title(\"Distribution of max number of hours in ICU for patients who develop Sespis\")\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHK2wQzmnEdZ"
      },
      "source": [
        "# Compare temperature between sepsis patient and non sepsis patient \n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Box(y=patientsdf.loc[patientsdf['max_SepsisLabel'] == 1]['max_Temperature'], name='Sepsis patient Temperature'))\n",
        "\n",
        "fig.add_trace(go.Box(y=patientsdf.loc[patientsdf['max_SepsisLabel'] == 0]['max_Temperature'], name='Non Sepsis patient Temperature'))\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSOZFbm3UlAC"
      },
      "source": [
        "# Compare heart rate between sepsis patient and non sepsis patient \n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Box(y=patientsdf.loc[patientsdf['max_SepsisLabel'] == 1]['max_HeartRate'], name='Sepsis patient Heart Rate'))\n",
        "\n",
        "fig.add_trace(go.Box(y=patientsdf.loc[patientsdf['max_SepsisLabel'] == 0]['max_HeartRate'], name='Non Sepsis patient Heart Rate'))\n",
        "\n",
        "fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMRBd0YMEveV"
      },
      "source": [
        "# Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IQdt2tImfns"
      },
      "source": [
        "# Defining pipeline \n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "category_LR = train_X.dtypes == object\n",
        "\n",
        "cat_pipeline = make_pipeline(OneHotEncoder(handle_unknown = \"ignore\"))\n",
        "num_pipeline = make_pipeline(SimpleImputer(strategy = 'median'), StandardScaler())\n",
        "\n",
        "preprocessor = make_column_transformer((num_pipeline, ~category_LR), (cat_pipeline, category_LR))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsqHPlmSFA49"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "# Training the algorithm  \n",
        "log_pip_scale = make_pipeline(preprocessor, LogisticRegression(solver = 'lbfgs', n_jobs = -1, C=0.01, random_state = 0, max_iter= 1000) )\n",
        "\n",
        "logModel = log_pip_scale.fit(train_X, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP_J2nhPSJIA"
      },
      "source": [
        "# Predict the test set result \n",
        "y_pred = logModel.predict(val_X)\n",
        "acc = accuracy_score(val_y, y_pred)\n",
        "\n",
        "# Confusion Matrix \n",
        "conf_matrix = confusion_matrix(val_y, y_pred)\n",
        "\n",
        "# Print result \n",
        "print('Prediction value counts:')\n",
        "print(pd.Series(y_pred).value_counts())\n",
        "print('\\nAccuracy on the training data is {0:f}'.format(acc))\n",
        "conf_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tBIHs5cguam"
      },
      "source": [
        "print(classification_report(val_y, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZfhmIrN3gBp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDdJj0WTja0G"
      },
      "source": [
        "# visualizing confusion Matrix using heatmap \n",
        "class_name = [0,1]\n",
        "fig, ax = plt.subplots()\n",
        "tick_mark = np.arange(len(class_name))\n",
        "plt.xticks(tick_mark, class_name)\n",
        "plt.yticks(tick_mark, class_name)\n",
        "\n",
        "sns.heatmap(pd.DataFrame(conf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('confusion matrix', y = 1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4FPXo1IlkwY"
      },
      "source": [
        "print(\"Accuracy:\", accuracy_score(val_y, y_pred))\n",
        "print(\"Precision:\", precision_score(val_y, y_pred))\n",
        "print(\"Recall:\", recall_score(val_y, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksxar6OWFFaV"
      },
      "source": [
        "# Decision Tree Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QyBhl5FPpF"
      },
      "source": [
        "# from sklearn.compose import ColumnTransformer\n",
        "# # Training the algorithm  \n",
        "# dtm_pip_scale = make_pipeline(preprocessor, tree.DecisionTreeClassifier() )\n",
        "\n",
        "# dtModel = dtm_pip_scale.fit(train_X, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqNFsD-oSOfH"
      },
      "source": [
        "# # Predict the test set result \n",
        "# y_pred = dtModel.predict(val_X)\n",
        "# acc = accuracy_score(val_y, y_pred)\n",
        "\n",
        "# # Confusion Matrix \n",
        "# conf_matrix = confusion_matrix(val_y, y_pred)\n",
        "\n",
        "# # Print result \n",
        "# print('Prediction value counts:')\n",
        "# print(pd.Series(y_pred).value_counts())\n",
        "# print('\\nAccuracy on the training data is {0:f}'.format(acc))\n",
        "# conf_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoqeFYA_VlWe"
      },
      "source": [
        "# # get data using pipeline \n",
        "# cat_names = preprocessor.fit(train_X).named_transformers_['pipeline-2'].named_steps['onehotencoder']\n",
        "\n",
        "# feature_list = list(cat_names.get_feature_names())\n",
        "# feature_list.extend(['ICULOS','HospAdmTime'])\n",
        "\n",
        "# import graphviz \n",
        "# dot_data = tree.export_graphviz(tree.DecisionTreeClassifier(max_leaf_nodes = 15).fit(preprocessor.fit_transform(train_X), train_y),\n",
        "#                                 out_file=none, feature_names = feature_list, class_name = ['normal','sepsis'],\n",
        "#                                 rounded = True, proportion = False, precision = 2)\n",
        "# graph = graphviz.Source(dot_data)\n",
        "# graph.render(\"iris\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKcWh5YwskeA"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "decision_tree_classifier = DecisionTreeClassifier()\n",
        "DTree = decision_tree_classifier.fit(train_X,train_y)\n",
        "predictedDT = decision_tree_classifier.predict(val_X)\n",
        "before_fs=metrics.accuracy_score(predictedDT,val_y)*100\n",
        "print(\"Accuracy using DecisionTreeClassifier:\",before_fs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMZqBYHX3iGA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gagl8MPd3idF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oaOjr273ivY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj06QALH2U0L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}